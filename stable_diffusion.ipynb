{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQqvQTUvhnGIy1tYy5u9De",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sonam-pankaj95/Language-Model/blob/main/stable_diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "6LlOazLfcsxJ",
        "outputId": "a9751622-3cfd-4ce3-db94-a0888b79bb7a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-502ee37438e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m## Import the CLIP artifacts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCLIPTextModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIPTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoencoderKL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUNet2DConditionModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLMSDiscreteScheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m## Initiating tokenizer and encoder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import torch, logging\n",
        "## disable warnings\n",
        "logging.disable(logging.WARNING)  \n",
        "## Imaging  library\n",
        "from PIL import Image\n",
        "from torchvision import transforms as tfms\n",
        "## Basic libraries\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import display\n",
        "import shutil\n",
        "import os\n",
        "## For video display\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "## Import the CLIP artifacts \n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n",
        "## Initiating tokenizer and encoder.\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n",
        "## Initiating the VAE\n",
        "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", torch_dtype=torch.float16).to(\"cuda\")\n",
        "## Initializing a scheduler and Setting number of sampling steps\n",
        "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "scheduler.set_timesteps(50)\n",
        "## Initializing the U-Net model\n",
        "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n",
        "## Helper functions\n",
        "def load_image(p):\n",
        "    '''\n",
        "    Function to load images from a defined path\n",
        "    '''\n",
        "    return Image.open(p).convert('RGB').resize((512,512))\n",
        "def pil_to_latents(image):\n",
        "    '''\n",
        "    Function to convert image to latents\n",
        "    '''\n",
        "    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0\n",
        "    init_image = init_image.to(device=\"cuda\", dtype=torch.float16) \n",
        "    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215\n",
        "    return init_latent_dist\n",
        "def latents_to_pil(latents):\n",
        "    '''\n",
        "    Function to convert latents to images\n",
        "    '''\n",
        "    latents = (1 / 0.18215) * latents\n",
        "    with torch.no_grad():\n",
        "        image = vae.decode(latents).sample\n",
        "    image = (image / 2 + 0.5).clamp(0, 1)\n",
        "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "    images = (image * 255).round().astype(\"uint8\")\n",
        "    pil_images = [Image.fromarray(image) for image in images]\n",
        "    return pil_images\n",
        "def text_enc(prompts, maxlen=None):\n",
        "    '''\n",
        "    A function to take a texual promt and convert it into embeddings\n",
        "    '''\n",
        "    if maxlen is None: maxlen = tokenizer.model_max_length\n",
        "    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\") \n",
        "    return text_encoder(inp.input_ids.to(\"cuda\"))[0].half()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_2_img(prompts, g=7.5, seed=100, steps=70, dim=512, save_int=False):\n",
        "    \"\"\"\n",
        "    Diffusion process to convert prompt to image\n",
        "    \"\"\"\n",
        "    \n",
        "    # Defining batch size\n",
        "    bs = len(prompts) \n",
        "    \n",
        "    # Converting textual prompts to embedding\n",
        "    text = text_enc(prompts) \n",
        "    \n",
        "    # Adding an unconditional prompt , helps in the generation process\n",
        "    uncond =  text_enc([\"\"] * bs, text.shape[1])\n",
        "    emb = torch.cat([uncond, text])\n",
        "    \n",
        "    # Setting the seed\n",
        "    if seed: torch.manual_seed(seed)\n",
        "    \n",
        "    # Initiating random noise\n",
        "    latents = torch.randn((bs, unet.in_channels, dim//8, dim//8))\n",
        "    \n",
        "    # Setting number of steps in scheduler\n",
        "    scheduler.set_timesteps(steps)\n",
        "    \n",
        "    # Adding noise to the latents \n",
        "    latents = latents.to(\"cuda\").half() * scheduler.init_noise_sigma\n",
        "    \n",
        "    # Iterating through defined steps\n",
        "    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n",
        "        # We need to scale the i/p latents to match the variance\n",
        "        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n",
        "        \n",
        "        # Predicting noise residual using U-Net\n",
        "        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n",
        "            \n",
        "        # Performing Guidance\n",
        "        pred = u + g*(t-u)\n",
        "        \n",
        "        # Conditioning  the latents\n",
        "        latents = scheduler.step(pred, ts, latents).prev_sample\n",
        "        \n",
        "        # Saving intermediate images\n",
        "        if save_int: \n",
        "            if not os.path.exists(f'./steps'):\n",
        "                os.mkdir(f'./steps')\n",
        "            latents_to_pil(latents)[0].save(f'steps/{i:04}.jpeg')\n",
        "            \n",
        "    # Returning the latent representation to output an image of 3x512x512\n",
        "    return latents_to_pil(latents)"
      ],
      "metadata": {
        "id": "m4RVVFVKc1mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = prompt_2_img([\"A dog wearing a hat\", \"a photograph of an astronaut riding a horse\"], save_int=False)\n",
        "for img in images:display(img)"
      ],
      "metadata": {
        "id": "4m-55B6Xc5B0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}